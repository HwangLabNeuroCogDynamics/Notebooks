{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the preprocessing command line tool, navigate to the preprocessing directory at /mnt/nfs/lss/lss_kahwang_hpc/scripts/preprocessing and run the command below in your terminal to display the documentation for the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python3 main.py --help\n",
    "```\n",
    "&nbsp;\n",
    "```\n",
    "usage: [DATASET_DIR] [SUBCOMMANDS [OPTIONS]]\n",
    "\n",
    "Run pre-processing on whole dataset or selected subjects\n",
    "\n",
    "Required Arguments:\n",
    "  dataset_dir           Base directory of dataset.\n",
    "  -h, --help            show this help message and exit\n",
    "\n",
    "Subcommands:\n",
    "  {heudiconv,mriqc,fmriprep,3dDeconvolve,regressors,3dmema,FD_stats}\n",
    "    heudiconv           Convert raw data files to BIDS format. Conversion script filepath is required.\n",
    "    mriqc               Run mriqc on dataset to analyze quality of data.\n",
    "    fmriprep            Preprocess data with fmriprep pipeline.\n",
    "    3dDeconvolve        Parse regressor files, censor motion, create stimfiles, and run 3dDeconvolve.\n",
    "    regressors          Parse regressor files to extract columns and censor motion.\n",
    "    3dmema              Runs 3dmema.\n",
    "    FD_stats            Calculates FD statistics for dataset. Outputs csv with % of points over FD threshold anbd FD mean for each run and subject.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the program takes in the dataset directory as an input, followed by a subcommand and it's options. We can look at each subcommands options by running python3 main.py dataset_dir/ {subcommand} --help  \n",
    "Let's try that below with fmriprep and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python3 main.py dataset_dir fmriprep --help\n",
    "```\n",
    "\n",
    "&nbsp;\n",
    "```\n",
    "usage: [OPTIONS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --fmriprep_opt FMRIPREP_OPT\n",
    "                        Options to add to fmriprep. Write between '' and replace - with * as shown: '**[OPTION1] arg1 ** [OPTION2] ...'\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "\n",
    "General Optional Arguments:\n",
    "  --rerun_mem           Rerun subjects that failed due to memory constraints\n",
    "  --slots SLOTS         Set number of slots/threads per subject. Default is 4.\n",
    "\n",
    "Argon HPC Optional Arguments:\n",
    "  --email               Receive email notifications from HPC\n",
    "  --no_qsub             Does not submit generated bash scripts.\n",
    "  --hold_jid HOLD_JID   Jobs will be placed on hold until specified job completes. [JOB_ID]\n",
    "  --no_resubmit         Enable to not resubmit tasks after migration. Default is to resubmit.\n",
    "  --mem MEM             Set memory for HPC\n",
    "  -q QUEUE, --queue QUEUE\n",
    "                        Set queue for HPC\n",
    "  --stack STACK STACK   Queue jobs in dependent stacks. When all jobs complete, next will start. Two required integer arguments [# of stacks][# of jobs per stack]. Use 'split' in second argument to split remaining jobs\n",
    "                        evenly amongst number of stacks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweetness. We can see what options are available for the fmriprep subcommand. Looks like there are a bunch of optional arguments. If you ever forget how to run a command or what options are available use the --help flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing python program automatically creates a bash script and then runs/submits (differs based on thalamege or argon host) the script. The program uses base bash scripts (thalamege: preprocessing/thalamege, argon: preprocessing/argon) and then fills in data based on user inputs. The new bash script will be written to either preprocessing/thalemege/dataset_dir_name or preprocessing/argon/jobs/dataset_dir_name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program records output info in the logs/ directory of each command directory (ie fmriprep/logs/). If you are running into issues or errors, you should check out the log files.  \n",
    "Additionally, the preprocessing pipeline automatically keeps track of completed subjects in the completed_subjects.txt file and failed subjects in the failed_subjects.txt. This is useful for datasets with subjects comtinually being added such as ThalHi. You simply run the command normally without specifying subjects and it will only run subjects that have not been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "General Optional Arguments:\n",
    "  --rerun_mem           Rerun subjects that failed due to memory constraints\n",
    "  --slots SLOTS         Set number of slots/threads per subject. Default is 4.\n",
    "```\n",
    "\n",
    "Rerun memory option reruns all subjects that are in failed_subjects_mem.txt file in /logs directory.  \n",
    "Slots specifies number of slots to run per subject. This is equivalent to the number of cores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset (minues completed subjects) by default or on number of subjects given via the --numsub flag\n",
    "```\n",
    "\n",
    "This might be the option you use the most. Pretty self explanatory.  \n",
    "For subjects, this is what the flag would look like:  \n",
    "--subjects 10001 10002\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "```\n",
    "\n",
    "Use the bids_dir flag when the bids directory is not in your root dataset directory. For example, the hcp developmental dataset is stored on a shared directory in argon so I would do --bids_dir /Dedicated/inc_data/bigdata/hcpd  \n",
    "The work_dir flag will change what working directory the pipeline will use. Mostly useful for argon and changing between working on localscratch, nfscratch, and lss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Argon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Argon HPC Optional Arguments:\n",
    "  --email               Receive email notifications from HPC\n",
    "  --no_qsub             Does not submit generated bash scripts to be run.\n",
    "  --hold_jid HOLD_JID   Jobs will be placed on hold until specified job completes. [JOB_ID]\n",
    "  --no_resubmit         Enable to not resubmit tasks after job migration. Default is to resubmit. This should be enabled when running on all.q\n",
    "  --mem MEM             Set memory for HPC\n",
    "  -q QUEUE, --queue QUEUE\n",
    "                        Set queue for HPC. Default is our queue: SEASHORE\n",
    "  --stack STACKS JOBS_PER_STACK   Queue jobs in dependent stacks. When all jobs complete, next will start. Two required integer arguments [# of stacks][# of jobs per stack]. Use 'split' in second argument to split remaining jobs evenly amongst number of stacks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting on Argon vs Thalamege"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program has some slight differences when submitting on argon vs thalamege. On argon, jobs are submitted to the SGE scheduler and will generally be submitted as task arrays split up by subject. On thalamege, the jobs run in parallel again split up by subject.  \n",
    "The program automatically knows which host you are on, so don't worry about having to tell it anything about the host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On argon, for some of jobs, especially fmriprep, it makes sense to first copy over data into localscratch to make the job run faster. The localscratch is local memory and is accessed by argon much faster than our network lss drive. Up the job finishing, any output data is copied over to the target output directory. Localscratch is used as the working directory by default on Argon and it will be used automatically for fmriprep. The localscratch storage is much smaller so for subjects with lots of sessions, you may run into issues running out of file storage. Simply change the working directory to one on the lss if this happens.  \n",
    "Check out the base fmriprep script to see an example of how that's done preprocessing/argon/fmriprep_base.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout fmripreps options by using the --help flag.\n",
    "\n",
    "```\n",
    "$ python3 main.py dataset_dir/ fmriprep --help\n",
    "usage: [OPTIONS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --fmriprep_opt FMRIPREP_OPT\n",
    "                        Options to add to fmriprep. Write between '' and replace - with * as shown: '**[OPTION1] arg1 **[OPTION2] ...'\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset (minus completed subjects)\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset (minus completed subjects) by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "\n",
    "General Optional Arguments:\n",
    "  --rerun_mem           Rerun subjects that failed due to memory constraints\n",
    "  --slots SLOTS         Set number of slots/threads per subject. Default is 4.\n",
    "\n",
    "Argon HPC Optional Arguments:\n",
    "  --email               Receive email notifications from HPC\n",
    "  --no_qsub             Does not submit generated bash scripts.\n",
    "  --hold_jid HOLD_JID   Jobs will be placed on hold until specified job completes. [JOB_ID]\n",
    "  --no_resubmit         Enable to not resubmit tasks after migration. Default is to resubmit.\n",
    "  --mem MEM             Set memory for HPC\n",
    "  -q QUEUE, --queue QUEUE\n",
    "                        Set queue for HPC\n",
    "  --stack STACK STACK   Queue jobs in dependent stacks. When all jobs complete, next will start. Two required integer arguments [# of stacks][# of jobs per stack]. Use 'split' in second argument to split remaining jobs evenly amongst number of stacks.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the documentation, fmriprep has all the standard options and only one unique optional argument called --fmriprep_opt. This flag is for adding options to running the fmriprep pipeline. An example call to run fmriprep in our generated bash script is below. The fmriprep options would be added after the last line.\n",
    "\n",
    "```\n",
    "singularity run --cleanenv -B $working_dataset_dir $singularity_path \\\n",
    "$working_bids_dir \\\n",
    "$working_dataset_dir \\\n",
    "participant --participant_label $subject \\\n",
    "--nthreads $slots --omp-nthreads $slots \\\n",
    "-w $working_dir \\\n",
    "--fs-license-file ${freesurfer_lic} \\\n",
    "--mem $10 \\\n",
    "--skip_bids_validation \\\n",
    "```\n",
    "\n",
    "We have to use a special syntax for writing these options. The computer would get confused if we used dashes (-) and so we need to replace them with stars (*) and do it between single quotes ('). Entering the command:  \n",
    "```\n",
    "python3 main.py dataset_dir/ fmriprep --fmriprep_opt '**stop*on*first*crash'\n",
    "```\n",
    "\n",
    "would produce this (only the last line is changed)\n",
    "```\n",
    "singularity run --cleanenv -B $working_dataset_dir $singularity_path \\\n",
    "$working_bids_dir \\\n",
    "$working_dataset_dir \\\n",
    "participant --participant_label $subject \\\n",
    "--nthreads $slots --omp-nthreads $slots \\\n",
    "-w $working_dir \\\n",
    "--fs-license-file ${freesurfer_lic} \\\n",
    "--mem $10 \\\n",
    "--skip_bids_validation \\\n",
    "--stop-on-first-crash\n",
    "```\n",
    "\n",
    "You could also just generate the bash script, use the --no_qsub so the script doesn't run, and then add the options manually, change anything you want, and run the generated script yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical fmriprep command will look like this:\n",
    "```\n",
    "python3 main.py dataset_dir/ fmriprep\n",
    "```\n",
    "\n",
    "Easy peasy. The program will automatically run any subjects that aren't completed yet (in logs/completed_subjects.txt) or have failed previously. And remember you can also run specific subjects using --subjects using --numsub\n",
    "```\n",
    "python3 main.py dataset_dir/ fmriprep --subjects 10001\n",
    "python3 main.py dataset_dir/ fmriprep --subjects 10001 10002\n",
    "python3 main.py dataset_dir/ fmriprep --numsub 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fmriprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mriqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the options for mriqc using the --help flag. (I hope you're noticing a pattern. Documentation is your best friend)\n",
    "\n",
    "```\n",
    "$ python3 main.py dataset_dir/ mriqc --help\n",
    "usage: [OPTIONS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --group               Run group analysis for mriqc instead of default participant level\n",
    "  --mriqc_opt MRIQC_OPT\n",
    "                        Options to add to mriqc. Write between '' as shown: '--[OPTION1] --[OPTION2] ...'\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset (minus completed subjects)\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset (minus completed subjects) by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "\n",
    "General Optional Arguments:\n",
    "  --rerun_mem           Rerun subjects that failed due to memory constraints\n",
    "  --slots SLOTS         Set number of slots/threads per subject. Default is 4.\n",
    "\n",
    "Argon HPC Optional Arguments:\n",
    "  --email               Receive email notifications from HPC\n",
    "  --no_qsub             Does not submit generated bash scripts.\n",
    "  --hold_jid HOLD_JID   Jobs will be placed on hold until specified job completes. [JOB_ID]\n",
    "  --no_resubmit         Enable to not resubmit tasks after migration. Default is to resubmit.\n",
    "  --mem MEM             Set memory for HPC\n",
    "  -q QUEUE, --queue QUEUE\n",
    "                        Set queue for HPC\n",
    "  --stack STACK STACK   Queue jobs in dependent stacks. When all jobs complete, next will start. Two required integer arguments [# of stacks][# of jobs per stack]. Use 'split' in second argument to split remaining jobs evenly amongst number of stacks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, mriqc is very similar to fmriprep with only the addition being the --group flag. Use this flag when you want to do group analysis for mriqc instead of the standard participant level analysis.  \n",
    "Similar to fmriprep you have the options flag, --mriqc_opt, to add options to running mriqc. The special syntax is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of your calls are going to look like this.\n",
    "```\n",
    "python3 main.py dataset_dir/ mriqc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressors command parses columns from regressors.tsv files and censors motion. The outputs will be nuisance.1D and censor.1D files in the 3dDeconvolve directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python3 preprocessing/main.py dataset_dir regressors --help\n",
    "usage: [OPTIONS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --regressors_wc REGRESSORS_WC\n",
    "                        Wildcard used to find regressors files using glob. Must have * at beggining. Default is *regressors.tsv.\n",
    "  -c [COLUMNS [COLUMNS ...]], --columns [COLUMNS [COLUMNS ...]]\n",
    "                        Enter columns to parse from regressors file into nuisance.1D file for usage in 3dDeconvolve. Default columns will be added automatically.\n",
    "  --no_default          Enter flag to not use default columns. If not entered, default columns will be parsed. Default columns are: ['csf', 'white_matter', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "  --threshold THRESHOLD\n",
    "                        Threshold for censoring. Default is 0.2\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset (minus completed subjects)\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset (minus completed subjects) by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional arguments\n",
    "- You can add columns to parse from the regressors files via the --columns flag\n",
    "- If you don't want default columns use the --no_default flag.\n",
    "- The --threhsold sets the threshold for censoring motion based on framewise displacement. The default is 0.2. You may have to lower this if too much data is being removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3dDeconvolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3dDeconvolve command will parse regressor files for default or given columns, censor motion, create stimulus timing files, and generate a 3dDeconvolve bash script. Outputs will be in the 3dDeconvolve/ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python3 main.py dataset_dir/ 3dDeconvolve --help\n",
    "usage: [stimulus_col][timing_col][OPTIONS]\n",
    "\n",
    "positional arguments:\n",
    "  stimulus_col          Column name for stimulus type in run timing file.\n",
    "  timing_col            Column name for time of stimulus presentation in run timing file.\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --bold_wc BOLD_WC     Wildcard used to find bold files using glob. Must have * at beggining. Default is *\n",
    "  --timing_file_dir TIMING_FILE_DIR\n",
    "                        Directory holding run timing files. Default is dataset BIDS directory.\n",
    "  --run_timing_wc RUN_TIMING_WC\n",
    "                        Wildcard used to find run timing files using glob. Must have * at beggining. Default is *\n",
    "  --regressors_wc REGRESSORS_WC\n",
    "                        Wildcard used to find regressors files using glob. Must have * at beggining. Default is *regressors.tsv.\n",
    "  --use_stimfiles       Use stimfiles instead of stim config for setting up 3dDeconvolve script.\n",
    "  -c [COLUMNS [COLUMNS ...]], --columns [COLUMNS [COLUMNS ...]]\n",
    "                        Enter columns to parse from regressors file into nuisance.1D file for usage in 3dDeconvolve. Default columns will be added automatically.\n",
    "  --no_default          Enter flag to not use default columns. If not entered, default columns will be parsed. Default columns are: ['csf', 'white_matter', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "  --sessions [SESSIONS [SESSIONS ...]]\n",
    "                        Set the sessions to be analyzed in order. Default will be all sessions in alphabetical order\n",
    "  --threshold THRESHOLD\n",
    "                        Threshold for censoring. Default is 0.2\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset (minus completed subjects)\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset (minus completed subjects) by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "\n",
    "General Optional Arguments:\n",
    "  --rerun_mem           Rerun subjects that failed due to memory constraints\n",
    "  --slots SLOTS         Set number of slots/threads per subject. Default is 4.\n",
    "\n",
    "Argon HPC Optional Arguments:\n",
    "  --email               Receive email notifications from HPC\n",
    "  --no_qsub             Does not submit generated bash scripts.\n",
    "  --hold_jid HOLD_JID   Jobs will be placed on hold until specified job completes. [JOB_ID]\n",
    "  --no_resubmit         Enable to not resubmit tasks after migration. Default is to resubmit.\n",
    "  --mem MEM             Set memory for HPC\n",
    "  -q QUEUE, --queue QUEUE\n",
    "                        Set queue for HPC\n",
    "  --stack STACK STACK   Queue jobs in dependent stacks. When all jobs complete, next will start. Two required integer arguments [# of stacks][# of jobs per stack]. Use 'split' in second argument to split remaining jobs evenly amongst number of stacks.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3dDeconvolve differs from our other commands as it has positional, required arguments. You must enter the stimulus_col and timing_col for this command to work. These are the column names for your stimuli and their timings. Generally found in the events.tsv file or behavorial data.  \n",
    "These two columns are used to generate stimfiles, which are required for 3dDeconvolve.  \n",
    "\n",
    "There are many optional arguments that are also specific and important to running 3dDeconvolve.  \n",
    "- The wildcards are useful when trying to pull out specific files, such as only a certain task.  \n",
    "- The --timing_file_dir is used to find behavorial data files storing stimulus and timing info. By default the program looks for *events.tsv in the fmriprep directory. For us, we sometimes keep this info in the rdss and this flag can be used to find those timing files.\n",
    "- The --sessions flag is useful when you have various sessions such as task and resting-state. You generally aren't going to want to mix the two, so you can use this flag to pull out only task data or only resting-state.  \n",
    "\n",
    "The other optional arguments --columns, --no_default, and --threshold I have explained in the regressors portion of this notebook and are used to specify columns to parse from regressors file for denoising and for setting a threshold to censor motion.  \n",
    "All others are common flags that I explained in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show an example of 3dDeconvolve on our ThalHi dataset. Below is the correct call.\n",
    "```\n",
    "python main.py /data/backed_up/shared/ThalHi_MRI_2020 3dDeconvolve Trial_type Time_Since_Run_Cue_Prez --timing_file_dir /mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data\n",
    "```\n",
    "\n",
    "Let's break it down. The first argument after main.py is our dataset directory, which points to the ThalHi 2020 MRI data on thalamege. Next, is our subcommand 3dDeconvolve. After this we get our two positional arguments: stimulus_col and timing_col.  \n",
    "In this case, we want to run 3dDeconvolve on each cue in the ThalHi task (Stay, EDS, IDS) so we look in our behavioral data () and find the corresponding column name to be Trial_type. Next, is the timing_col which represents the timing of when the stimuli occurred. This column name is Time_Since_Run_Cue_Prez. \n",
    "Finally, the events/behavioral data is on the rdss instead of the 'default' fmriprep directory, so we need to specify that directory by using the --timing_file_dir flag.\n",
    "\n",
    "Below is a snapshot of the output you will see. The 3dDeconvolve command will parse regressor files for default or given columns, censor motion, and create stimulus timing files. All outputs will be in the 3dDeconvolve/ folder. Like fmriprep and mriqc, it will also generate a bash script. You can use/edit this or just use your own. This program is most useful for generating the stimfiles, parsing regressors, and censoring motion.\n",
    "```\n",
    "Prepping 3dDeconvolve on subject 10003\n",
    "\n",
    "\n",
    "Parsing regressor files for subject 10003 in /data/backed_up/shared/ThalHi_MRI_2020/fmriprep/sub-10003/\n",
    "Parsing: sub-10003_task-ThalHi_run-1_desc-confounds_regressors.tsv\n",
    "Parsing: sub-10003_task-ThalHi_run-2_desc-confounds_regressors.tsv\n",
    "Parsing: sub-10003_task-ThalHi_run-3_desc-confounds_regressors.tsv\n",
    "Parsing: sub-10003_task-ThalHi_run-4_desc-confounds_regressors.tsv\n",
    "Parsing: sub-10003_task-ThalHi_run-5_desc-confounds_regressors.tsv\n",
    "Parsing: sub-10003_task-ThalHi_run-6_desc-confounds_regressors.tsv\n",
    "Parsing: sub-10003_task-ThalHi_run-7_desc-confounds_regressors.tsv\n",
    "Parsing: sub-10003_task-ThalHi_run-8_desc-confounds_regressors.tsv\n",
    "Writing regressor file to /data/backed_up/shared/ThalHi_MRI_2020/3dDeconvolve/sub-10003/nuisance.1D\n",
    "Writing censor file to /data/backed_up/shared/ThalHi_MRI_2020/3dDeconvolve/sub-10003/censor.1D\n",
    "\n",
    "\n",
    "Successfully extracted columns ['csf', 'white_matter', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z'] from regressor files and censored motion\n",
    "['/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_001_Task_THHS_2020_Aug_21_0817.csv', '/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_002_Task_THHS_2020_Aug_21_0826.csv', '/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_003_Task_THHS_2020_Aug_21_0833.csv', '/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_004_Task_THHS_2020_Aug_21_0841.csv', '/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_005_Task_THHS_2020_Aug_21_0851.csv', '/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_006_Task_THHS_2020_Aug_21_0900.csv', '/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_007_Task_THHS_2020_Aug_21_0907.csv', '/mnt/cifs/rdss/rdss_kahwang/ThalHi_data/MRI_data/Behavioral_data/10003_008_Task_THHS_2020_Aug_21_0915.csv']\n",
    "\n",
    "Creating stimulus files for subject 10003\n",
    "Writing stimulus file: Stay\n",
    "Writing stimulus file: EDS\n",
    "Writing stimulus file: IDS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heudiconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heudiconv is part of the preprocessing command line tool. Let's look at its input options by inputting the --help flag after the heudiconv command.**  \n",
    "**Example shown below.**\n",
    "\\\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "```\n",
    "python main.py dataset_dir/ heudiconv --help\n",
    "\n",
    "usage: [SCRIPT_PATH][OPTIONS]\n",
    "\n",
    "positional arguments:\n",
    "  script_path           Filename of script. Script must be located in following directory: /data/backed_up/shared/bin/heudiconv/heuristics/\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --post_conv_script POST_CONV_SCRIPT\n",
    "                        Filepath of post-heudiconv Conversion script. Ocassionally needed to make further changes after running heudiconv.\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see from the documentation that the the heudiconv command takes 1 required input (SCRIPT_PATH) and some optional flags.** \\\n",
    "**The Script Path refers to the python script used to run heudiconv.**   \n",
    "```\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sometimes with Heudiconv you need to make changes to data after running heuidconv, you can specify a post conversion script with the --post_conv_script flag.**  \n",
    "**For example, for ThalHi our post conversion script can be found at /mnt/nfs/lss/lss_kahwang_hpc/scripts/thalhi/heudiconv_post.py. The heudiconv command would then be:** \n",
    "```\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py --post_conv_script /mnt/nfs/lss/lss_kahwang_hpc/scripts/thalhi/heudiconv_post.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Like our other preproccessing commands, we can specify which subjects to run or how many to run using the --subjects and --numsub flags.**\n",
    "```\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py --subjects 10001 10002\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py --numsub 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The script will run in parallel on each subject.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FD_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python383jvsc74a57bd0b945bfa426ab19dcb1b57a95042a567490cf91a191c6db7383bb4e52050ebd91"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "b945bfa426ab19dcb1b57a95042a567490cf91a191c6db7383bb4e52050ebd91"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}