{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the preprocessing command line tool, navigate to the preprocessing directory at /mnt/nfs/lss/lss_kahwang_hpc/scripts/preprocessing and run the command below in your terminal to display the documentation for the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python3 main.py --help\n",
    "```\n",
    "&nbsp;\n",
    "```\n",
    "usage: [DATASET_DIR] [SUBCOMMANDS [OPTIONS]]\n",
    "\n",
    "Run pre-processing on whole dataset or selected subjects\n",
    "\n",
    "Required Arguments:\n",
    "  dataset_dir           Base directory of dataset.\n",
    "  -h, --help            show this help message and exit\n",
    "\n",
    "Subcommands:\n",
    "  {heudiconv,mriqc,fmriprep,3dDeconvolve,regressors,3dmema,FD_stats}\n",
    "    heudiconv           Convert raw data files to BIDS format. Conversion script filepath is required.\n",
    "    mriqc               Run mriqc on dataset to analyze quality of data.\n",
    "    fmriprep            Preprocess data with fmriprep pipeline.\n",
    "    3dDeconvolve        Parse regressor files, censor motion, create stimfiles, and run 3dDeconvolve.\n",
    "    regressors          Parse regressor files to extract columns and censor motion.\n",
    "    3dmema              Runs 3dmema.\n",
    "    FD_stats            Calculates FD statistics for dataset. Outputs csv with % of points over FD threshold anbd FD mean for each run and subject.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the program takes in the dataset directory as an input, followed by a subcommand and it's options. We can look at each subcommands options by running python3 main.py dataset_dir/ {subcommand} --help  \n",
    "Let's try that below with fmriprep and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python3 main.py dataset_dir fmriprep --help\n",
    "```\n",
    "\n",
    "&nbsp;\n",
    "```\n",
    "usage: [OPTIONS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --fmriprep_opt FMRIPREP_OPT\n",
    "                        Options to add to fmriprep. Write between '' and replace - with * as shown: '**[OPTION1] arg1 ** [OPTION2] ...'\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "\n",
    "General Optional Arguments:\n",
    "  --rerun_mem           Rerun subjects that failed due to memory constraints\n",
    "  --slots SLOTS         Set number of slots/threads per subject. Default is 4.\n",
    "\n",
    "Argon HPC Optional Arguments:\n",
    "  --email               Receive email notifications from HPC\n",
    "  --no_qsub             Does not submit generated bash scripts.\n",
    "  --hold_jid HOLD_JID   Jobs will be placed on hold until specified job completes. [JOB_ID]\n",
    "  --no_resubmit         Enable to not resubmit tasks after migration. Default is to resubmit.\n",
    "  --mem MEM             Set memory for HPC\n",
    "  -q QUEUE, --queue QUEUE\n",
    "                        Set queue for HPC\n",
    "  --stack STACK STACK   Queue jobs in dependent stacks. When all jobs complete, next will start. Two required integer arguments [# of stacks][# of jobs per stack]. Use 'split' in second argument to split remaining jobs\n",
    "                        evenly amongst number of stacks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweetness. We can see what options are available for the fmriprep subcommand. Looks like there are a bunch of optional arguments. If you ever forget how to run a command or what options are available use the --help flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing python program automatically creates a bash script and then runs/submits (differs based on thalamege or argon host) the script. The program uses base bash scripts (thalamege: preprocessing/thalamege, argon: preprocessing/argon) and then fills in data based on user inputs. The new bash script will be written to either preprocessing/thalemege/dataset_dir_name or preprocessing/argon/jobs/dataset_dir_name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program records output info in the logs/ directory of each command directory (ie fmriprep/logs/). If you are running into issues or errors, you should check out the log files.  \n",
    "Additionally, the preprocessing pipeline automatically keeps track of completed subjects in the completed_subjects.txt file and failed subjects in the failed_subjects.txt. This is useful for datasets with subjects comtinually being added such as ThalHi. You simply run the command normally without specifying subjects and it will only run subjects that have not been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "General Optional Arguments:\n",
    "  --rerun_mem           Rerun subjects that failed due to memory constraints\n",
    "  --slots SLOTS         Set number of slots/threads per subject. Default is 4.\n",
    "```\n",
    "\n",
    "Rerun memory option reruns all subjects that are in failed_subjects_mem.txt file in /logs directory.  \n",
    "Slots specifies number of slots to run per subject. This is equivalent to the number of cores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset (minues completed subjects) by default or on number of subjects given via the --numsub flag\n",
    "```\n",
    "\n",
    "This might be the option you use the most. Pretty self explanatory.  \n",
    "For subjects, this is what the flag would look like:  \n",
    "--subjects 10001 10002\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "```\n",
    "\n",
    "Use the bids_dir flag when the bids directory is not in your root dataset directory. For example, the hcp developmental dataset is stored on a shared directory in argon so I would do --bids_dir /Dedicated/inc_data/bigdata/hcpd  \n",
    "The work_dir flag will change what working directory the pipeline will use. Mostly useful for argon and changing between working on localscratch, nfscratch, and lss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Argon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Argon HPC Optional Arguments:\n",
    "  --email               Receive email notifications from HPC\n",
    "  --no_qsub             Does not submit generated bash scripts to be run.\n",
    "  --hold_jid HOLD_JID   Jobs will be placed on hold until specified job completes. [JOB_ID]\n",
    "  --no_resubmit         Enable to not resubmit tasks after job migration. Default is to resubmit. This should be enabled when running on all.q\n",
    "  --mem MEM             Set memory for HPC\n",
    "  -q QUEUE, --queue QUEUE\n",
    "                        Set queue for HPC. Default is our queue: SEASHORE\n",
    "  --stack STACKS JOBS_PER_STACK   Queue jobs in dependent stacks. When all jobs complete, next will start. Two required integer arguments [# of stacks][# of jobs per stack]. Use 'split' in second argument to split remaining jobs evenly amongst number of stacks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting on Argon vs Thalamege"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program has some slight differences when submitting on argon vs thalamege. On argon, jobs are submitted to the SGE scheduler and will generally be submitted as task arrays split up by subject. On thalamege, the jobs run in parallel again split up by subject.  \n",
    "The program automatically knows which host you are on, so don't worry about having to tell it anything about the host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On argon, for some of jobs, especially fmriprep, it makes sense to first copy over data into localscratch to make the job run faster. The localscratch is local memory and is accessed by argon much faster than our network lss drive. Up the job finishing, any output data is copied over to the target output directory. Localscratch is used as the working directory by default on Argon and it will be used automatically for fmriprep. The localscratch storage is much smaller so for subjects with lots of sessions, you may run into issues running out of file storage. Simply change the working directory to one on the lss if this happens.  \n",
    "Check out the base fmriprep script to see an example of how that's done preprocessing/argon/fmriprep_base.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fmriprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mriqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3dDeconvolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heudiconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heudiconv is part of the preprocessing command line tool. Let's look at its input options by inputting the --help flag after the heudiconv command.**  \n",
    "**Example shown below.**\n",
    "\\\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "```\n",
    "python main.py dataset_dir/ heudiconv --help\n",
    "\n",
    "usage: [SCRIPT_PATH][OPTIONS]\n",
    "\n",
    "positional arguments:\n",
    "  script_path           Filename of script. Script must be located in following directory: /data/backed_up/shared/bin/heudiconv/heuristics/\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --post_conv_script POST_CONV_SCRIPT\n",
    "                        Filepath of post-heudiconv Conversion script. Ocassionally needed to make further changes after running heudiconv.\n",
    "\n",
    "Subject arguments:\n",
    "  -n NUMSUB, --numsub NUMSUB\n",
    "                        The number of subjects being analyzed. If none listed, default will be whole dataset\n",
    "  -s [SUBJECTS [SUBJECTS ...]], --subjects [SUBJECTS [SUBJECTS ...]]\n",
    "                        The subjects being analyzed. Do not include sub- prefix. If subjects are not included, pre-processing will be run on whole dataset by default or on number of subjects given via the --numsub flag\n",
    "\n",
    "Path arguments:\n",
    "  --bids_dir BIDS_DIR   Path for bids directory if not located in dataset directory.\n",
    "  --work_dir WORK_DIR   The working dir for programs. Default for argon is user dir in localscratch. Default for thalamege is work directory in dataset directory.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see from the documentation that the the heudiconv command takes 1 required input (SCRIPT_PATH) and some optional flags.** \\\n",
    "**The Script Path refers to the python script used to run heudiconv.**   \n",
    "```\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sometimes with Heudiconv you need to make changes to data after running heuidconv, you can specify a post conversion script with the --post_conv_script flag.**  \n",
    "**For example, for ThalHi our post conversion script can be found at /mnt/nfs/lss/lss_kahwang_hpc/scripts/thalhi/heudiconv_post.py. The heudiconv command would then be:** \n",
    "```\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py --post_conv_script /mnt/nfs/lss/lss_kahwang_hpc/scripts/thalhi/heudiconv_post.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Like our other preproccessing commands, we can specify which subjects to run or how many to run using the --subjects and --numsub flags.**\n",
    "```\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py --subjects 10001 10002\n",
    "python main.py dataset_dir/ heudiconv /data/backed_up/shared/bin/heudiconv/heuristics/{Script Name}.py --numsub 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The script will run in parallel on each subject.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FD_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python383jvsc74a57bd0b945bfa426ab19dcb1b57a95042a567490cf91a191c6db7383bb4e52050ebd91"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "b945bfa426ab19dcb1b57a95042a567490cf91a191c6db7383bb4e52050ebd91"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}